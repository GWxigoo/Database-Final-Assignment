import re
import csv
import pandas as pd
from urllib.parse import unquote_plus

# =========================================================
# CONFIG
# =========================================================
INPUT_PATH = "/content/SQL_Injection_Detection_Dataset.csv"
OUTPUT_FILE = "final_sqli_feature_engineered.xlsx"

# =========================================================
# 1) LOAD DATASET (robust for messy CSV)
# =========================================================
def load_csv_robust(path: str) -> pd.DataFrame:
    try:
        return pd.read_csv(path)
    except Exception as e:
        print("⚠️ Default read_csv failed, fallback to python engine + skip bad lines.")
        print("Reason:", repr(e))
        return pd.read_csv(
            path,
            engine="python",
            on_bad_lines="skip",
            dtype=str,
            keep_default_na=False
        )

df = load_csv_robust(INPUT_PATH)
df = df.rename(columns=lambda c: c.strip())

# Ensure Query column exists
if "Query" not in df.columns:
    lower_map = {c.lower().strip(): c for c in df.columns}
    if "query" in lower_map:
        df["Query"] = df[lower_map["query"]]
    else:
        raise ValueError(f"Cannot find 'Query' column. Columns: {list(df.columns)}")

df["Query"] = df["Query"].astype(str)

# =========================================================
# 2) BASIC LENGTH FEATURES
# =========================================================
df["query_len"] = df["Query"].str.len()

# =========================================================
# 3) FEATURE EXTRACTION (RAW QUERY) — EXPANDED
# =========================================================

def has(pattern, text):
    return int(bool(re.search(pattern, text, re.IGNORECASE | re.DOTALL)))

# -------------------------
# Boolean-based injection
# -------------------------
df["is_boolean"] = df["Query"].apply(
    lambda q: has(
        r"\b(and|or)\b|"          # AND / OR
        r"&&|\|\||"               # && ||
        r"\b(and|or)\W*\d",       # AND 1
        q
    )
)

# -------------------------
# Always-true (FIXED + EXPANDED)
# -------------------------
df["always_true"] = df["Query"].apply(
    lambda q: has(
        r"\b\d+\s*=\s*\d+\b|"                 # 1=1
        r"\b\d+\s*<\s*\d+\b|"                 # 1<2
        r"\b\d+\s*>\s*\d+\b|"                 # 2>1
        r"\b\d+\s*<=\s*\d+\b|"                # 1<=1
        r"\b\d+\s*>=\s*\d+\b|"                # 1>=1
        r"('([^']+)'\s*=\s*'\2')|"            # 'a'='a'
        r"(\"([^\"]+)\"\s*=\s*\"\4\")|"       # "a"="a"
        r"\b(length|len|count)\s*\(.*\)\s*>\s*0",  # length(x)>0
        q
    )
)

# -------------------------
# UNION-based injection
# -------------------------
df["is_union"] = df["Query"].apply(
    lambda q: has(
        r"\bunion\b[\s\/\*]*\b(all\s+)?select\b",
        q
    )
)

# -------------------------
# Comment truncation
# -------------------------
df["is_comment"] = df["Query"].apply(
    lambda q: has(
        r"--|--\+|#|/\*",
        q
    )
)

# -------------------------
# Time-based blind SQLi
# -------------------------
df["is_time_based"] = df["Query"].apply(
    lambda q: has(
        r"sleep\s*\(|pg_sleep\s*\(|"
        r"benchmark\s*\(|"
        r"waitfor\s+delay|"
        r"dbms_lock\.sleep|"
        r"if\s*\(.*sleep|"
        r"case\s+when.*sleep",
        q
    )
)

# -------------------------
# Comparison operator count (expanded)
# -------------------------
COMPARE_OP_RE = re.compile(r"<=|>=|<>|!=|=|<|>")
df["comparison_op_count"] = df["Query"].apply(
    lambda q: len(COMPARE_OP_RE.findall(q))
)

# -------------------------
# Parentheses depth
# -------------------------
def parentheses_depth(q: str) -> int:
    depth = max_depth = 0
    for c in q:
        if c == "(":
            depth += 1
            max_depth = max(max_depth, depth)
        elif c == ")":
            depth = max(0, depth - 1)
    return max_depth

df["parentheses_depth"] = df["Query"].apply(parentheses_depth)

# -------------------------
# Logical operator count (expanded)
# -------------------------
df["logic_operator_count"] = df["Query"].apply(
    lambda q: len(
        re.findall(r"\b(and|or)\b|&&|\|\|", q, re.IGNORECASE)
    )
)

# =========================================================
# 4) DELETE ROWS WITH ALL-ZERO FEATURES
# =========================================================
feature_cols = [
    "is_boolean", "always_true", "is_union", "is_comment",
    "is_time_based",
    "comparison_op_count", "parentheses_depth", "logic_operator_count"
]

df = df[df[feature_cols].sum(axis=1) > 0].copy()
print(" Remaining rows after all-zero filter:", len(df))

# =========================================================
# 5) NORMALIZATION
# =========================================================
def predecode(q: str) -> str:
    q = str(q).replace("\x00", " ")
    for _ in range(2):
        new_q = unquote_plus(q)
        if new_q == q:
            break
        q = new_q
    return q

def normalize_sql(q: str) -> str:
    q = predecode(q).lower().strip()
    q = re.sub(r"\s+", " ", q)

    q = re.sub(r"/\*.*?\*/", " /* */ ", q, flags=re.DOTALL)
    q = re.sub(r"--[^\r\n]*", " -- ", q)
    q = re.sub(r"#[^\r\n]*", " # ", q)

    q = re.sub(r"\[[^\]]+\]", "<id>", q)
    q = re.sub(r"`[^`]+`", "<id>", q)
    q = re.sub(r"'[^']*'", "<str>", q)
    q = re.sub(r'"[^"]*"', "<str>", q)
    q = re.sub(r"\b\d+(\.\d+)?\b", "<num>", q)

    q = re.sub(r"\s*\)\s*=\s*\(\s*", " = ", q)
    q = re.sub(r"\s*==\s*", " = ", q)

    return q

df["normalized_query"] = df["Query"].apply(normalize_sql)

# =========================================================
# 6) TOKENIZATION (STRING-ONLY) 
# =========================================================
TOKEN_PATTERN = re.compile(r"<str>|<num>|<id>|\b[a-z_]+\b")

SQL_VOCAB = {
    "select","union","from","where","and","or","insert","update","delete",
    "drop","sleep","pg_sleep","benchmark","waitfor","delay",
    "information_schema","pg_catalog","sqlite_master","sysobjects","sysdatabases",
    "<num>","<str>","<id>"
}

def tokenize_and_filter(q: str):
    tokens = TOKEN_PATTERN.findall(q)
    return [t for t in tokens if t in SQL_VOCAB]

df["filtered_tokens"] = df["normalized_query"].apply(tokenize_and_filter)
df["filtered_tokens_str"] = df["filtered_tokens"].apply(lambda x: " ".join(x))

# =========================================================
# 7) STR / NUM COUNTS
# =========================================================
df["str_count"] = df["normalized_query"].apply(lambda q: q.count("<str>"))
df["num_count"] = df["normalized_query"].apply(lambda q: q.count("<num>"))

# =========================================================
# 8) EXCEL-SAFE SANITIZATION
# =========================================================
ILLEGAL_EXCEL_CHARS = re.compile(r"[\x00-\x08\x0B-\x0C\x0E-\x1F]")

def remove_illegal_excel_chars(text):
    if isinstance(text, str):
        return ILLEGAL_EXCEL_CHARS.sub("", text)
    return text

for col in ["Query", "normalized_query", "filtered_tokens_str"]:
    df[col] = df[col].apply(remove_illegal_excel_chars)

# =========================================================
# 9) EXPORT
# =========================================================
df.to_excel(OUTPUT_FILE, index=False)
print(f" Final Excel exported: {OUTPUT_FILE}")
print("Remaining rows:", len(df))
